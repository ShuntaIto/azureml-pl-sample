{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1621410542431
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extension horovod.torch has not been built: /anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-38-x86_64-linux-gnu.so not found\n",
            "If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
            "Warning! MPI libs are missing, but python applications are still avaiable.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import io\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from transformers import BertJapaneseTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "import pandas as pd\n",
        "import tarfile\n",
        "from glob import glob\n",
        "import linecache\n",
        "from tqdm import tqdm\n",
        "import urllib.request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall process: 0 of 8855190\n",
            "Overall process: 2 of 8855190\n",
            "Overall process: 10 of 8855190\n",
            "Overall process: 8202 of 8855190\n",
            "Overall process: 9822 of 8855190\n",
            "Overall process: 12075 of 8855190\n",
            "Overall process: 14755 of 8855190\n",
            "Overall process: 17736 of 8855190\n",
            "Overall process: 20047 of 8855190\n",
            "Overall process: 22602 of 8855190\n",
            "Overall process: 24866 of 8855190\n",
            "Overall process: 27150 of 8855190\n",
            "Overall process: 29386 of 8855190\n",
            "Overall process: 31578 of 8855190\n",
            "Overall process: 33814 of 8855190\n",
            "Overall process: 36322 of 8855190\n",
            "Overall process: 38863 of 8855190\n",
            "Overall process: 41277 of 8855190\n",
            "Overall process: 43376 of 8855190\n",
            "Overall process: 45992 of 8855190\n",
            "Overall process: 48098 of 8855190\n",
            "Overall process: 50424 of 8855190\n",
            "Overall process: 52671 of 8855190\n",
            "Overall process: 54976 of 8855190\n",
            "Overall process: 57346 of 8855190\n",
            "Overall process: 59584 of 8855190\n",
            "Overall process: 62011 of 8855190\n",
            "Overall process: 63941 of 8855190\n",
            "Overall process: 66429 of 8855190\n",
            "Overall process: 68647 of 8855190\n",
            "Overall process: 70726 of 8855190\n",
            "Overall process: 73374 of 8855190\n",
            "Overall process: 75734 of 8855190\n",
            "Overall process: 78238 of 8855190\n",
            "Overall process: 80488 of 8855190\n",
            "Overall process: 82889 of 8855190\n",
            "Overall process: 85189 of 8855190\n",
            "Overall process: 87430 of 8855190\n",
            "Overall process: 89772 of 8855190\n",
            "Overall process: 92026 of 8855190\n",
            "Overall process: 94294 of 8855190\n",
            "Overall process: 96497 of 8855190\n",
            "Overall process: 98527 of 8855190\n",
            "Overall process: 100693 of 8855190\n",
            "Overall process: 103000 of 8855190\n",
            "Overall process: 105367 of 8855190\n",
            "Overall process: 107464 of 8855190\n",
            "Overall process: 109786 of 8855190\n",
            "Overall process: 112082 of 8855190\n",
            "Overall process: 114495 of 8855190\n",
            "Overall process: 116481 of 8855190\n",
            "Overall process: 118558 of 8855190\n",
            "Overall process: 121026 of 8855190\n",
            "Overall process: 123472 of 8855190\n",
            "Overall process: 125577 of 8855190\n",
            "Overall process: 127456 of 8855190\n",
            "Overall process: 129443 of 8855190\n",
            "Overall process: 131564 of 8855190\n",
            "Overall process: 133911 of 8855190\n",
            "Overall process: 136366 of 8855190\n",
            "Overall process: 138603 of 8855190\n",
            "Overall process: 140869 of 8855190\n",
            "Overall process: 143074 of 8855190\n",
            "Overall process: 145281 of 8855190\n",
            "Overall process: 147402 of 8855190\n",
            "Overall process: 149868 of 8855190\n",
            "Overall process: 151968 of 8855190\n",
            "Overall process: 154322 of 8855190\n",
            "Overall process: 156273 of 8855190\n",
            "Overall process: 158356 of 8855190\n",
            "Overall process: 160622 of 8855190\n",
            "Overall process: 162699 of 8855190\n",
            "Overall process: 164853 of 8855190\n",
            "Overall process: 166888 of 8855190\n",
            "Overall process: 169132 of 8855190\n",
            "Overall process: 171600 of 8855190\n",
            "Overall process: 173606 of 8855190\n",
            "Overall process: 175785 of 8855190\n",
            "Overall process: 178055 of 8855190\n",
            "Overall process: 180612 of 8855190\n",
            "Overall process: 182733 of 8855190\n",
            "Overall process: 185274 of 8855190\n",
            "Overall process: 187302 of 8855190\n",
            "Overall process: 189409 of 8855190\n",
            "Overall process: 191867 of 8855190\n",
            "Overall process: 193946 of 8855190\n",
            "Overall process: 196107 of 8855190\n",
            "Overall process: 198604 of 8855190\n",
            "Overall process: 200595 of 8855190\n",
            "Overall process: 202829 of 8855190\n",
            "Overall process: 205155 of 8855190\n",
            "Overall process: 207256 of 8855190\n",
            "Overall process: 209047 of 8855190\n",
            "Overall process: 211203 of 8855190\n",
            "Overall process: 213207 of 8855190\n",
            "Overall process: 215225 of 8855190\n",
            "Overall process: 217634 of 8855190\n",
            "Overall process: 219668 of 8855190\n",
            "Overall process: 221988 of 8855190\n",
            "Overall process: 224062 of 8855190\n",
            "Overall process: 226290 of 8855190\n",
            "Overall process: 228492 of 8855190\n",
            "Overall process: 230759 of 8855190\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1ba58f89f3e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTarFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_file_progress_file_object_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mon_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mtar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mProgressFileObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/tarfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, numeric_owner)\u001b[0m\n\u001b[1;32m   2024\u001b[0m                 \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0o700\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2025\u001b[0m             \u001b[0;31m# Do not set_attrs directories, as we will do that further down\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2026\u001b[0;31m             self.extract(tarinfo, path, set_attrs=not tarinfo.isdir(),\n\u001b[0m\u001b[1;32m   2027\u001b[0m                          numeric_owner=numeric_owner)\n\u001b[1;32m   2028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/tarfile.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, member, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2066\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2067\u001b[0;31m             self._extract_member(tarinfo, os.path.join(path, tarinfo.name),\n\u001b[0m\u001b[1;32m   2068\u001b[0m                                  \u001b[0mset_attrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_attrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2069\u001b[0m                                  numeric_owner=numeric_owner)\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/tarfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/tarfile.py\u001b[0m in \u001b[0;36mmakefile\u001b[0;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[1;32m   2178\u001b[0m         \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m         \u001b[0mbufsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopybufsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2180\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mbltn_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2181\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "download_path = \"ldcc-20140209.tar.gz\"\n",
        "extract_path = \"livedoor/\"\n",
        "\n",
        "def track_progress(members):\n",
        "   for member in members:\n",
        "      # this will be the current file being extracted\n",
        "      yield member\n",
        "\n",
        "if not os.path.isfile(download_path):\n",
        "    urllib.request.urlretrieve(\"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\",download_path)\n",
        "\n",
        "\n",
        "## https://stackoverflow.com/questions/3667865/python-tarfile-progress-output\n",
        "def get_file_progress_file_object_class(on_progress):\n",
        "    class FileProgressFileObject(tarfile.ExFileObject):\n",
        "        def read(self, size, *args):\n",
        "            on_progress(self.name, self.position, self.size)\n",
        "            return tarfile.ExFileObject.read(self, size, *args)\n",
        "    return FileProgressFileObject\n",
        "\n",
        "class TestFileProgressFileObject(tarfile.ExFileObject):\n",
        "    def read(self, size, *args):\n",
        "        on_progress(self.name, self.position, self.size)\n",
        "        return tarfile.ExFileObject.read(self, size, *args)\n",
        "\n",
        "class ProgressFileObject(io.FileIO):\n",
        "    def __init__(self, path, *args, **kwargs):\n",
        "        self._total_size = os.path.getsize(path)\n",
        "        io.FileIO.__init__(self, path, *args, **kwargs)\n",
        "\n",
        "    def read(self, size):\n",
        "        print(\"Overall process: %d of %d\" %(self.tell(), self._total_size))\n",
        "        return io.FileIO.read(self, size)\n",
        "\n",
        "def on_progress(filename, position, total_size):\n",
        "    print(\"%s: %d of %s\" %(filename, position, total_size))\n",
        "\n",
        "tarfile.TarFile.fileobject = get_file_progress_file_object_class(on_progress)\n",
        "tar = tarfile.open(fileobj=ProgressFileObject(download_path))\n",
        "tar.extractall(extract_path)\n",
        "tar.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>中川翔子さん、城田優さん、D・DATEによる「ラテ研」　D・DATEはとにかくキャメルが好き\\n</td>\n",
              "      <td>peachy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>癒しの空間で大人気の「オリーブスパ」は、いい刺激で溢れている職場\\n</td>\n",
              "      <td>peachy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>「転職できる力」養成講座　その9 ： 『図で考えるとすべてまとまる 』\\n</td>\n",
              "      <td>livedoor-homme</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>“モデル別で見る”東京ガールズコレクション 2011 スナップレポートNo.1\\n</td>\n",
              "      <td>peachy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ドコモ、薄さ6.7mmの極薄スマホを発売へ！ドコモ、「MEDIAS ES N-05D」が凄い...</td>\n",
              "      <td>it-life-hack</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title        category\n",
              "0   中川翔子さん、城田優さん、D・DATEによる「ラテ研」　D・DATEはとにかくキャメルが好き\\n          peachy\n",
              "1                 癒しの空間で大人気の「オリーブスパ」は、いい刺激で溢れている職場\\n          peachy\n",
              "2              「転職できる力」養成講座　その9 ： 『図で考えるとすべてまとまる 』\\n  livedoor-homme\n",
              "3          “モデル別で見る”東京ガールズコレクション 2011 スナップレポートNo.1\\n          peachy\n",
              "4  ドコモ、薄さ6.7mmの極薄スマホを発売へ！ドコモ、「MEDIAS ES N-05D」が凄い...    it-life-hack"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## https://qiita.com/m__k/items/841950a57a0d7ff05506\n",
        "\n",
        "categories = [name for name in os.listdir(extract_path + \"text\") if os.path.isdir(extract_path + \"text/\" + name)]\n",
        "\n",
        "datasets = pd.DataFrame(columns=[\"title\", \"category\"])\n",
        "for cat in categories:\n",
        "    path = extract_path + \"text/\" + cat + \"/*.txt\"\n",
        "    files = glob(path)\n",
        "    for text_name in files:\n",
        "        title = linecache.getline(text_name, 3)\n",
        "        s = pd.Series([title, cat], index=datasets.columns)\n",
        "        datasets = datasets.append(s, ignore_index=True)\n",
        "\n",
        "datasets = datasets.sample(frac=1).reset_index(drop=True)\n",
        "datasets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>category_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NTTドコモ、MEDIAS PP N-01Dにて特定のブラウザを利用したときに不具合でソフト...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>大切な投稿を見逃さない　Facebookに親友の投稿だけを表示する【知っ得！虎の巻】\\n</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>大画面スマホ時代はデジタル・ドクショが一気に快適に！大幅リニューアルで使える便利すぎる機能満...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>おはこんこん、ふぉっくす紺子です！アキバで流れる動画が決まりました【紺子にゅうす】\\n</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>雑誌をPDF化してiPadで読む裏技！スキャナー活用のノウハウを伝授【新スタイル活用術】\\n</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  category_id\n",
              "0  NTTドコモ、MEDIAS PP N-01Dにて特定のブラウザを利用したときに不具合でソフト...            0\n",
              "1       大切な投稿を見逃さない　Facebookに親友の投稿だけを表示する【知っ得！虎の巻】\\n            2\n",
              "2  大画面スマホ時代はデジタル・ドクショが一気に快適に！大幅リニューアルで使える便利すぎる機能満...            0\n",
              "3        おはこんこん、ふぉっくす紺子です！アキバで流れる動画が決まりました【紺子にゅうす】\\n            2\n",
              "4     雑誌をPDF化してiPadで読む裏技！スキャナー活用のノウハウを伝授【新スタイル活用術】\\n            2"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## https://qiita.com/m__k/items/e312ddcf9a3d0ea64d72\n",
        "\n",
        "categories = list(set(datasets['category']))\n",
        "id2cat = dict(zip(list(range(len(categories))), categories))\n",
        "cat2id = dict(zip(categories, list(range(len(categories)))))\n",
        "\n",
        "datasets['category_id'] = datasets['category'].map(cat2id)\n",
        "datasets = datasets[['title', 'category_id']]\n",
        "\n",
        "datasets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1fe845d575ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertJapaneseTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cl-tohoku/bert-base-japanese-whole-word-masking'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1693\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m                     resolved_vocab_files[file_id] = cached_path(\n\u001b[0m\u001b[1;32m   1696\u001b[0m                         \u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m                         \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1774\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_remote_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1776\u001b[0;31m         output_path = get_from_cache(\n\u001b[0m\u001b[1;32m   1777\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1778\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   2038\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{url} not found in cache or force_download set to True, downloading to {temp_file.name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2040\u001b[0;31m             \u001b[0mhttp_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"storing {url} in cache at {cache_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers)\u001b[0m\n\u001b[1;32m   1888\u001b[0m     \u001b[0mcontent_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresume_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcontent_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m     progress = tqdm(\n\u001b[0m\u001b[1;32m   1891\u001b[0m         \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m         \u001b[0munit_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0munit_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_printer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplayed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Prepare IPython progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mIProgress\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# #187 #451 #558 #872\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0;34m\"IProgress not found. Please update jupyter and ipywidgets.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;34m\" See https://ipywidgets.readthedocs.io/en/stable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
          ]
        }
      ],
      "source": [
        "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[2, 1325, 9, 12453, 2992, 8, 3]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.encode(\"私は元気です。\",padding=True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LivedoorDatasets(torch.utils.data.Dataset):\n",
        "    def __init__(self, transform = None):\n",
        "        self.transform = transform\n",
        "\n",
        "        self.data = list(datasets[\"title\"])\n",
        "        self.label = list(datasets[\"category_id\"])\n",
        "        \n",
        "        if not len(self.data) == len(self.label):\n",
        "            raise ValueError(\"Invalid dataset\")\n",
        "        self.datanum = len(self.data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.datanum\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        out_data = self.data[idx]\n",
        "        out_label = self.label[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            out_data = self.transform([\"input_ids\"][0])\n",
        "\n",
        "        return out_data, out_label   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "livedoor_datasets = LivedoorDatasets()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7376\n",
            "7376\n"
          ]
        }
      ],
      "source": [
        "print(len(list(datasets[\"category_id\"])))\n",
        "print(len(list(datasets[\"title\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('NTTドコモ、MEDIAS PP N-01Dにて特定のブラウザを利用したときに不具合でソフトウェア更新を提供開始\\n', 0)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "livedoor_datasets[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TokenizerCollate:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        input = [item[0] for item in batch]\n",
        "        input = self.tokenizer(\n",
        "            input,\n",
        "            padding=True,\n",
        "            max_length=512,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\")\n",
        "        targets = torch.tensor([item[1] for item in batch])\n",
        "        return input, targets\n",
        "    \n",
        "    def __call__(self, batch):\n",
        "        return self.collate_fn(batch)\n",
        "\n",
        "train_loader = DataLoader(livedoor_datasets, batch_size=16, collate_fn=TokenizerCollate(tokenizer=tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Model(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
        "        self.output = nn.Linear(768, 9)\n",
        "        \n",
        "        self.train_acc = pl.metrics.Accuracy()\n",
        "        self.val_acc = pl.metrics.Accuracy()\n",
        "        self.test_acc = pl.metrics.Accuracy()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.bert(**x).last_hidden_state\n",
        "        ## cls token相当部分のhidden_stateのみ抜粋\n",
        "        y = y[:,0,:]\n",
        "        y = y.view(-1, 768)\n",
        "        y = self.output(y)\n",
        "        return y\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        x, t = batch\n",
        "        y = self(x)\n",
        "        loss = F.cross_entropy(y, t)\n",
        "        self.log(\"loss\", loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_nb): \n",
        "        x, t = batch\n",
        "        y = self(x)\n",
        "        loss = F.cross_entropy(y, t)\n",
        "        preds = torch.argmax(y, dim=1)\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "        self.log('val_acc', self.val_acc(y,t), prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_nb):\n",
        "        return self.validation_step(batch, batch_nb)\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=0.02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "for param in model.bert.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n"
          ]
        }
      ],
      "source": [
        "trainer = pl.Trainer(gpus=1,max_epochs=1,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/py38-pt180/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: you defined a validation_step but have no val_dataloader. Skipping validation loop\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name      | Type      | Params\n",
            "----------------------------------------\n",
            "0 | bert      | BertModel | 110 M \n",
            "1 | output    | Linear    | 6.9 K \n",
            "2 | train_acc | Accuracy  | 0     \n",
            "3 | val_acc   | Accuracy  | 0     \n",
            "4 | test_acc  | Accuracy  | 0     \n",
            "----------------------------------------\n",
            "6.9 K     Trainable params\n",
            "110 M     Non-trainable params\n",
            "110 M     Total params\n",
            "442.497   Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "860c88cebdc548a6978679c204d67cfd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation sanity check: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/py38-pt180/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3977a56d35b24dd4b4fbc49e05898f4d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.fit(model, train_loader) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summarize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in train_loader:\n",
        "    print(i[0].shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "814635fff90d4fb7edb7cf7b4efbafc2586e819c8eea62750f0795f97ef3e14f"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.6 - AzureML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 3
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
