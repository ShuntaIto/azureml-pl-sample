{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0955a9e46ddbd40183737b0407937cba8340df4839a34630809b37eda4d8ebe94",
   "display_name": "Python 3.8.8 64-bit ('py38-pt180': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from transformers import BertJapaneseTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "from glob import glob\n",
    "import linecache\n",
    "from tqdm import tqdm\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_path = \"livedoor_news_corpus.tar.gz\"\n",
    "extract_path = \"livedoor/\"\n",
    "\n",
    "if not os.path.isfile(\"livedoor_news_corpus.tar.gz\"):\n",
    "    urllib.request.urlretrieve(\"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\",download_path)\n",
    "\n",
    "with tarfile.open(download_path, \"r:gz\") as t:\n",
    "    t.extractall(extract_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://qiita.com/m__k/items/841950a57a0d7ff05506\n",
    "\n",
    "categories = [name for name in os.listdir(extract_path + \"text\") if os.path.isdir(extract_path + \"text/\" + name)]\n",
    "\n",
    "datasets = pd.DataFrame(columns=[\"title\", \"category\"])\n",
    "for cat in categories:\n",
    "    path = extract_path + \"text/\" + cat + \"/*.txt\"\n",
    "    files = glob(path)\n",
    "    for text_name in files:\n",
    "        title = linecache.getline(text_name, 3)\n",
    "        s = pd.Series([title, cat], index=datasets.columns)\n",
    "        datasets = datasets.append(s, ignore_index=True)\n",
    "\n",
    "datasets = datasets.sample(frac=1).reset_index(drop=True)\n",
    "datasets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://qiita.com/m__k/items/e312ddcf9a3d0ea64d72\n",
    "\n",
    "categories = list(set(datasets['category']))\n",
    "id2cat = dict(zip(list(range(len(categories))), categories))\n",
    "cat2id = dict(zip(categories, list(range(len(categories)))))\n",
    "\n",
    "datasets['category_id'] = datasets['category'].map(cat2id)\n",
    "datasets = datasets[['title', 'category_id']]\n",
    "\n",
    "datasets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"私は元気です。\",padding=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LivedoorDatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform = None):\n",
    "        self.transform = transform\n",
    "\n",
    "        self.data = list(datasets[\"title\"])\n",
    "        self.label = list(datasets[\"category_id\"])\n",
    "        \n",
    "        if not len(self.data) == len(self.label):\n",
    "            raise ValueError(\"Invalid dataset\")\n",
    "        self.datanum = len(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "        out_label = self.label[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            out_data = self.transform([\"input_ids\"][0])\n",
    "\n",
    "        return out_data, out_label   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "livedoor_datasets = LivedoorDatasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(datasets[\"category_id\"])))\n",
    "print(len(list(datasets[\"title\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "livedoor_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerCollate:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        input = [item[0] for item in batch]\n",
    "        input = self.tokenizer(\n",
    "            input,\n",
    "            padding=True,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\")\n",
    "        targets = torch.tensor([item[1] for item in batch])\n",
    "        return input[\"input_ids\"], targets\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        return self.collate_fn(batch)\n",
    "\n",
    "train_loader = DataLoader(livedoor_datasets, batch_size=16, collate_fn=TokenizerCollate(tokenizer=tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "        self.output = nn.Linear(768, 9)\n",
    "        \n",
    "        self.train_acc = pl.metrics.Accuracy()\n",
    "        self.val_acc = pl.metrics.Accuracy()\n",
    "        self.test_acc = pl.metrics.Accuracy()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.bert(x).last_hidden_state\n",
    "        ## cls token相当部分のhidden_stateのみ抜粋\n",
    "        y = y[:,0,:]\n",
    "        y = y.view(-1, 768)\n",
    "        y = self.output(y)\n",
    "        return y\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, t = batch\n",
    "        y = self(x)\n",
    "        loss = F.cross_entropy(y, t)\n",
    "        self.log(\"loss\", loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb): \n",
    "        x, t = batch\n",
    "        y = self(x)\n",
    "        loss = F.cross_entropy(y, t)\n",
    "        preds = torch.argmax(y, dim=1)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc(y,t), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        return self.validation_step(batch, batch_nb)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(gpus=1,max_epochs=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, train_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_loader:\n",
    "    print(i[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}