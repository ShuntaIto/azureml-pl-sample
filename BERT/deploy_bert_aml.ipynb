{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from azureml.core import Workspace\n",
        "from azureml.core.conda_dependencies import CondaDependencies \n",
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core.environment import Environment\n",
        "from azureml.core.webservice import AciWebservice\n",
        "from azureml.core.model import Model\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import requests"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1621406695124
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()\n",
        "ws"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "Workspace.create(name='shuit-ml-workspace', subscription_id='902f236f-44df-463a-a5cb-1516ab2a9cd2', resource_group='shuit-common')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1621406720950
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pred/score.py\n",
        "\n",
        "# score.pyの出力\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import onnxruntime\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "\n",
        "from transformers import BertJapaneseTokenizer\n",
        "\n",
        "\n",
        "def init():\n",
        "    global session, input_ids_name, attention_mask_name, token_type_ids_name, output_name, tokenizer\n",
        "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
        "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
        "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
        "    model = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model.onnx')\n",
        "    session = onnxruntime.InferenceSession(model, None)\n",
        "    input_ids_name = session.get_inputs()[0].name\n",
        "    attention_mask_name = session.get_inputs()[1].name\n",
        "    token_type_ids_name = session.get_inputs()[2].name\n",
        "    output_name = session.get_outputs()[0].name \n",
        "    tokenizer = BertJapaneseTokenizer.from_pretrained(\n",
        "        'cl-tohoku/bert-base-japanese-whole-word-masking')\n",
        "    \n",
        "\n",
        "def preprocess(input_data_json):\n",
        "    # convert the JSON data into the tensor input\n",
        "    input = json.loads(input_data_json)['data']\n",
        "    print(input)\n",
        "    tokenized_input = tokenizer(\n",
        "        input,\n",
        "        padding=True,\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\")\n",
        "    input_ids = tokenized_input[\"input_ids\"].to('cpu').detach().numpy().copy()\n",
        "    attention_mask = tokenized_input[\"attention_mask\"].to('cpu').detach().numpy().copy()\n",
        "    token_type_ids = tokenized_input[\"token_type_ids\"].to('cpu').detach().numpy().copy()\n",
        "    print(input_ids)\n",
        "    return (input_ids, attention_mask, token_type_ids)\n",
        "\n",
        "def postprocess(result):\n",
        "    # We use argmax to pick the highest confidence label\n",
        "    return int(np.argmax(np.array(result).squeeze(), axis=0))\n",
        "    \n",
        "def run(input_data):\n",
        "\n",
        "    try:\n",
        "        # load in our data, convert to readable format\n",
        "        data = preprocess(input_data)\n",
        "        \n",
        "        # start timer\n",
        "        start = time.time()\n",
        "        \n",
        "        r = session.run([output_name], {input_ids_name: data[0], attention_mask_name:data[1], token_type_ids_name:data[2]})\n",
        "        print(r)\n",
        "        #end timer\n",
        "        end = time.time()\n",
        "        \n",
        "        result = postprocess(r)\n",
        "        result_dict = {\"result\": result,\n",
        "                      \"time_in_sec\": end - start}\n",
        "    except Exception as e:\n",
        "        result_dict = {\"error\": str(e)}\n",
        "    \n",
        "    return result_dict\n",
        "\n",
        "def choose_class(result_prob):\n",
        "    \"\"\"We use argmax to determine the right label to choose from our output\"\"\"\n",
        "    return int(np.argmax(result_prob, axis=0))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pred/score.py\n"
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 推論環境の定義ファイル生成と環境設定\n",
        "\n",
        "myenv = CondaDependencies.create(pip_packages=[\"numpy\", \"onnxruntime\", \"azureml-core\", \"azureml-defaults\", \"transformers\", \"fugashi\", \"ipadic\", \"torch\"])\n",
        "env_file_path = os.path.join(\"pred\", \"environment.yml\")\n",
        "score_file_path = os.path.join(\"pred\", \"score.py\")\n",
        "\n",
        "with open(env_file_path, \"w\") as f:\n",
        "    f.write(myenv.serialize_to_string())\n",
        "\n",
        "env = Environment.from_conda_specification(name=\"onnx_env\", file_path=env_file_path)\n",
        "inference_config = InferenceConfig(entry_script=score_file_path, environment=env)"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1621408230087
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ACI設定\n",
        "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
        "                                               memory_gb = 1, \n",
        "                                               tags = {'framework': 'onnx'}, \n",
        "                                               description = 'bert fine-tuned for livedoor news corpus')"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1621408230259
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデル指定\n",
        "model = Model(ws, 'bert-livedoor-model')"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1621408230919
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# デプロイ\n",
        "aci_service_name = 'bert-livedoor'\n",
        "print(\"Service\", aci_service_name)\n",
        "aci_service = Model.deploy(ws, aci_service_name, [model], inference_config, aciconfig)\n",
        "aci_service.wait_for_deployment(True)\n",
        "print(aci_service.state)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Service bert-livedoor\n",
            "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
            "Running\n",
            "2021-05-19 07:10:43+00:00 Creating Container Registry if not exists.\n",
            "2021-05-19 07:10:44+00:00 Registering the environment.\n",
            "2021-05-19 07:10:44+00:00 Use the existing image.\n",
            "2021-05-19 07:10:45+00:00 Generating deployment configuration.\n",
            "2021-05-19 07:10:45+00:00 Submitting deployment to compute..\n",
            "2021-05-19 07:10:52+00:00 Checking the status of deployment bert-livedoor..\n",
            "2021-05-19 07:15:28+00:00 Checking the status of inference endpoint bert-livedoor.\n",
            "Succeeded\n",
            "ACI service creation operation finished, operation \"Succeeded\"\n",
            "Healthy\n"
          ]
        }
      ],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1621408534555
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 推論\n",
        "endpoint = aci_service.scoring_uri\n",
        "input_data = json.dumps({'data': [\"この副題はどこに分類される？\"]})\n",
        "res = requests.post(url=endpoint, data=input_data, headers={'Content-Type': 'application/json'})\n",
        "res.json()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "{'result': 7, 'time_in_sec': 0.07662081718444824}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1621408535320
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"予測値: \"+str(res.json()[\"result\"]))\n",
        "#print(\"正解: \"+str(int(target_class)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "予測値: 7\n"
          ]
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1621408535892
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python388jvsc74a57bd0955a9e46ddbd40183737b0407937cba8340df4839a34630809b37eda4d8ebe94",
      "display_name": "Python 3.8.8 64-bit ('py38-pt180': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}