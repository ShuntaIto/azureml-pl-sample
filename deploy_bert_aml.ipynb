{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import Model\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pred/score.py\n",
    "\n",
    "# score.pyの出力\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from transformers import BertJapaneseTokenizer\n",
    "\n",
    "\n",
    "def init():\n",
    "    global session, input_name, output_name, tokenizer\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
    "    model = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model.onnx')\n",
    "    session = onnxruntime.InferenceSession(model, None)\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name \n",
    "    tokenizer = BertJapaneseTokenizer.from_pretrained(\n",
    "        'cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "    \n",
    "\n",
    "def preprocess(input_data_json):\n",
    "    # convert the JSON data into the tensor input\n",
    "    input = json.loads(input_data_json)['data']\n",
    "    print(input)\n",
    "    input = tokenizer(\n",
    "            input,\n",
    "            padding=True,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\")[\"input_ids\"]\n",
    "    input= input.to('cpu').detach().numpy().copy()\n",
    "    print(input)\n",
    "    return input\n",
    "\n",
    "def postprocess(result):\n",
    "    # We use argmax to pick the highest confidence label\n",
    "    return int(np.argmax(np.array(result).squeeze(), axis=0))\n",
    "    \n",
    "def run(input_data):\n",
    "\n",
    "    try:\n",
    "        # load in our data, convert to readable format\n",
    "        data = preprocess(input_data)\n",
    "        \n",
    "        # start timer\n",
    "        start = time.time()\n",
    "        \n",
    "        r = session.run([output_name], {input_name: data})\n",
    "        print(r)\n",
    "        #end timer\n",
    "        end = time.time()\n",
    "        \n",
    "        result = postprocess(r)\n",
    "        result_dict = {\"result\": result,\n",
    "                      \"time_in_sec\": end - start}\n",
    "    except Exception as e:\n",
    "        result_dict = {\"error\": str(e)}\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "def choose_class(result_prob):\n",
    "    \"\"\"We use argmax to determine the right label to choose from our output\"\"\"\n",
    "    return int(np.argmax(result_prob, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論環境の定義ファイル生成と環境設定\n",
    "\n",
    "myenv = CondaDependencies.create(pip_packages=[\"numpy\", \"onnxruntime\", \"azureml-core\", \"azureml-defaults\", \"transformers\", \"fugashi\", \"ipadic\", \"torch\"])\n",
    "env_file_path = os.path.join(\"pred\", \"environment.yml\")\n",
    "score_file_path = os.path.join(\"pred\", \"score.py\")\n",
    "\n",
    "with open(env_file_path, \"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())\n",
    "\n",
    "env = Environment.from_conda_specification(name=\"onnx_env\", file_path=env_file_path)\n",
    "inference_config = InferenceConfig(entry_script=score_file_path, environment=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACI設定\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
    "                                               memory_gb = 1, \n",
    "                                               tags = {'framework': 'onnx'}, \n",
    "                                               description = 'bert fine-tuned for livedoor news corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル指定\n",
    "model = Model(ws, 'bert-livedoor-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# デプロイ\n",
    "aci_service_name = 'bert-livedoor-api'\n",
    "print(\"Service\", aci_service_name)\n",
    "aci_service = Model.deploy(ws, aci_service_name, [model], inference_config, aciconfig)\n",
    "aci_service.wait_for_deployment(True)\n",
    "print(aci_service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論\n",
    "endpoint = aci_service.scoring_uri\n",
    "input_data = json.dumps({'data': [\"この副題はどこに分類される？\"]})\n",
    "res = requests.post(url=endpoint, data=input_data, headers={'Content-Type': 'application/json'})\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"予測値: \"+str(res.json()[\"result\"]))\n",
    "#print(\"正解: \"+str(int(target_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd0955a9e46ddbd40183737b0407937cba8340df4839a34630809b37eda4d8ebe94",
   "display_name": "Python 3.8.8 64-bit ('py38-pt180': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}